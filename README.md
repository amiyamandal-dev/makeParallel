# makeParallel ğŸš€

**High-performance Python decorators powered by Rust for true GIL-free parallelism**

[![Tests](https://img.shields.io/badge/tests-33%20passed-brightgreen)](test_all.py)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Rust](https://img.shields.io/badge/rust-2024-orange.svg)](https://www.rust-lang.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

Transform your CPU-bound Python code with simple decorators that unlock true parallel executionâ€”no GIL, no `multiprocessing` complexity.

## âœ¨ Features

- ğŸ”¥ **True Parallelism** - Rust threads without Python's Global Interpreter Lock
- ğŸ¯ **Simple API** - Just add a decorator, like `@parallel`
- âš¡ **High Performance** - Optimized with Crossbeam, Rayon, and DashMap
- ğŸ›¡ï¸ **Fault Tolerant** - Failed tasks don't crash others
- ğŸ“¦ **Zero Dependencies** - Pure Rust + Python, no external runtime
- ğŸ”§ **Production Ready** - Comprehensive test suite (33 tests passing)

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone <repo-url>
cd makeParallel

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install maturin

# Build and install
maturin develop --release
```

## ğŸš€ Quick Start

### Basic Parallel Execution

```python
from makeParallel import parallel

@parallel
def cpu_intensive(n):
    """This runs in a Rust thread without the GIL!"""
    return sum(i**2 for i in range(n))

# PUSH work to thread (non-blocking)
handle = cpu_intensive(1_000_000)

# CHECK status (non-blocking)
if handle.is_ready():
    print("Done!")

# PULL result (blocking)
result = handle.get()
print(f"Result: {result}")
```

### Performance Decorators

```python
from makeParallel import timer, log_calls, memoize, retry

# Measure execution time
@timer
def slow_function():
    time.sleep(1)
    return "done"

# Log all function calls
@log_calls
def process(x, y):
    return x + y

# Cache expensive computations
@memoize
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Retry on failure
@retry(max_retries=3)
def flaky_api_call():
    response = requests.get("https://api.example.com")
    return response.json()
```

### Call Tracking

```python
from makeParallel import CallCounter

@CallCounter
def tracked_function(x):
    return x * 2

tracked_function(5)
tracked_function(10)

print(f"Called {tracked_function.call_count} times")  # Output: Called 2 times

tracked_function.reset()  # Reset counter
```

## ğŸ¯ Available Decorators

### Core Decorators

| Decorator | Purpose | Example |
|-----------|---------|---------|
| `@parallel` | Run function in Rust thread | `@parallel def work(x): ...` |
| `@timer` | Measure execution time | `@timer def slow(): ...` |
| `@log_calls` | Log function calls and returns | `@log_calls def api(): ...` |
| `@CallCounter` | Track call count | `@CallCounter def func(): ...` |
| `@retry(max_retries=N)` | Retry on failure | `@retry(max_retries=3) def flaky(): ...` |
| `@memoize` | Cache results | `@memoize def expensive(x): ...` |

### Optimized Decorators

| Decorator | Technology | Best For |
|-----------|-----------|----------|
| `@parallel_fast` | Crossbeam channels | High throughput, many concurrent tasks |
| `@parallel_pool` | Rayon thread pool | Many small tasks (100+) |
| `@memoize_fast` | DashMap (lock-free) | High cache hit rate, concurrent access |
| `parallel_map(fn, items)` | Rayon par_iter | Batch processing, automatic load balancing |

## ğŸ“Š Performance Comparison

### vs Standard Python Threading

```python
import threading
import time

# Python threading (GIL-bound) âŒ
def python_threads():
    def work():
        sum(i**2 for i in range(1_000_000))

    threads = [threading.Thread(target=work) for _ in range(4)]
    start = time.time()
    for t in threads: t.start()
    for t in threads: t.join()
    print(f"Time: {time.time() - start:.2f}s")  # ~2.0s (sequential!)

# makeParallel (GIL-free) âœ…
from makeParallel import parallel

@parallel
def rust_parallel():
    return sum(i**2 for i in range(1_000_000))

start = time.time()
handles = [rust_parallel() for _ in range(4)]
results = [h.get() for h in handles]
print(f"Time: {time.time() - start:.2f}s")  # ~0.5s (4x speedup!)
```

### vs multiprocessing.Pool

```python
# multiprocessing âŒ (process overhead)
from multiprocessing import Pool

def task(x):
    return x ** 2

with Pool(4) as pool:
    results = pool.map(task, range(100))
# Overhead: process spawning, pickling, IPC

# makeParallel âœ… (zero overhead)
from makeParallel import parallel_map

results = parallel_map(lambda x: x ** 2, range(100))
# Overhead: none!
```

## ğŸ”¬ API Reference

### AsyncHandle Methods

When you call a `@parallel` decorated function, you get an `AsyncHandle`:

```python
@parallel
def task():
    return 42

handle = task()  # Returns AsyncHandle
```

#### `is_ready() -> bool`
Check if result is ready (non-blocking).

```python
if handle.is_ready():
    print("Task completed!")
```

#### `try_get() -> Optional[Any]`
Try to get result without blocking. Returns `None` if not ready.

```python
result = handle.try_get()
if result is None:
    print("Still working...")
```

#### `get() -> Any`
Block until result is ready and return it (safe to call multiple times).

```python
result = handle.get()  # Blocks until done
```

#### `wait(timeout_secs: Optional[float]) -> bool`
Wait for completion with optional timeout.

```python
if handle.wait(5.0):  # Wait up to 5 seconds
    result = handle.get()
```

#### `cancel() -> None`
Cancel the operation and clean up.

```python
handle.cancel()
```

## ğŸ“ Usage Examples

### Map-Reduce Pattern

```python
from makeParallel import parallel

@parallel
def square(x):
    return x ** 2

# Map
handles = [square(i) for i in range(100)]

# Reduce
results = [h.get() for h in handles]
total = sum(results)
```

### Batch Processing

```python
from makeParallel import parallel_map

def process_image(path):
    # ... expensive image processing
    return processed_image

# Process 1000 images in parallel
image_paths = ["img1.jpg", "img2.jpg", ...]
results = parallel_map(process_image, image_paths)
```

### Non-Blocking Workflow

```python
from makeParallel import parallel
import time

@parallel
def long_task():
    time.sleep(5)
    return "done"

handle = long_task()

# Do other work while task runs
while not handle.is_ready():
    print("Working...")
    time.sleep(0.5)
    # Do other stuff

result = handle.get()
```

### Fault-Tolerant Processing

```python
from makeParallel import parallel

@parallel
def might_fail(x):
    if x == 5:
        raise ValueError("Bad value!")
    return x * 2

handles = [might_fail(i) for i in range(10)]

for i, h in enumerate(handles):
    try:
        result = h.get()
        print(f"Task {i}: {result}")
    except Exception as e:
        print(f"Task {i} failed: {e}")
# Output: Tasks continue even if one fails âœ…
```

### Class Methods

```python
from makeParallel import parallel

class DataProcessor:
    def __init__(self, factor):
        self.factor = factor

    @parallel
    def process(self, data):
        return [x * self.factor for x in data]

processor = DataProcessor(10)
handle = processor.process([1, 2, 3])
result = handle.get()  # [10, 20, 30]
```

## ğŸ”§ Advanced Usage

### Combining Decorators

```python
from makeParallel import timer, log_calls, memoize

@timer          # Measure time
@log_calls      # Log calls
@memoize        # Cache results
def expensive_computation(x, y):
    time.sleep(1)
    return x ** y

# First call: logged, timed, cached
result1 = expensive_computation(2, 10)

# Second call: instant (from cache)
result2 = expensive_computation(2, 10)
```

### Choosing the Right Decorator

```python
# General purpose
@parallel
def general_task(data):
    return process(data)

# High throughput (crossbeam channels)
@parallel_fast
def high_volume_task(item):
    return quick_process(item)

# Many small tasks (rayon pool)
@parallel_pool
def small_task(x):
    return x * 2

handles = [small_task(i) for i in range(1000)]

# Batch processing (rayon par_iter)
from makeParallel import parallel_map
results = parallel_map(lambda x: x * 2, range(10000))
```

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Run all tests (33 tests)
python tests/test_all.py

# Run specific test files
python tests/test_parallel_simple.py
python tests/test_minimal.py

# Run benchmarks
python examples/benchmark_optimizations.py

# Prove true parallelism
python examples/prove_true_parallelism.py
```

## ğŸ“ˆ Benchmarks

From our benchmark suite:

```
4 tasks Ã— 2 seconds each:
- Sequential:     8.0s
- @parallel:      2.13s  (3.76x speedup) âœ…

100 concurrent tasks:
- std::mpsc:      0.2646s
- crossbeam:      0.2637s  (1.00x, lower latency)

200 small tasks:
- new threads:    0.2602s
- thread pool:    0.2577s  (1.01x, less overhead)
```

## ğŸ—ï¸ Architecture

```
Python Thread (Main)           Rust Threads (Workers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Call @parallel                Thread 1: Acquires GIL
  â†“                           Executes Python code
Release GIL                   Releases GIL
(py.allow_threads)                    â†“
                              Thread 2: Acquires GIL
                              Executes Python code
                              Releases GIL
                                      â†“
                              All threads run in parallel
                              (no GIL contention!)
                                      â†“
Wait for results â†â”€â”€â”€â”€â”€â”€â”€     Results via channel (mpsc/crossbeam)
  â†“
Get results
```

## ğŸ”¬ Technical Details

### Technologies Used

- **PyO3 0.27.1** - Rust â†” Python bindings
- **Crossbeam 0.8** - Lock-free MPMC channels
- **Rayon 1.10** - Work-stealing thread pool
- **DashMap 6.1** - Lock-free concurrent HashMap

### Why Rust?

1. **No GIL** - Rust threads run truly parallel
2. **Zero-cost abstractions** - No runtime overhead
3. **Memory safety** - No data races or segfaults
4. **Performance** - Native code speed

### Comparison Table

| Feature | makeParallel | threading | multiprocessing |
|---------|-------------|-----------|-----------------|
| GIL-free | âœ… | âŒ | âœ… |
| Startup overhead | None | None | High (process spawn) |
| Memory sharing | âœ… | âœ… | âŒ (requires pickling) |
| Fault tolerance | âœ… | âŒ | âœ… |
| Status checking | âœ… Non-blocking | âŒ | âŒ |
| Simple API | âœ… | Moderate | Complex |

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Run tests: `python tests/test_all.py`
2. Check formatting: `cargo fmt`
3. Run clippy: `cargo clippy`

## ğŸ“š Documentation

- **Full Guide**: See [OPTIMIZATION_GUIDE.md](docs/OPTIMIZATION_GUIDE.md)
- **Parallelism Proof**: See [PARALLELISM_PROOF.md](docs/PARALLELISM_PROOF.md)
- **API Reference**: See [README_PARALLEL.md](docs/README_PARALLEL.md)

## ğŸ¯ Roadmap

- [x] Thread pool size configuration âœ…
- [ ] AsyncIO integration
- [x] Priority queues âœ…
- [x] Task cancellation improvements âœ…
- [x] Performance profiling tools âœ…

## ğŸ†• Latest Features

### Enhanced Error Handling
Rich error context with full task information:
```python
try:
    result = handle.get()
except Exception as e:
    # Error includes: task_name, task_id, elapsed_time, error_type
    print(f"Task failed: {e}")
```

### Graceful Shutdown
Production-ready shutdown with automatic cleanup:
```python
import atexit
atexit.register(lambda: mp.shutdown(timeout_secs=30, cancel_pending=True))

# Or manual shutdown
success = mp.shutdown(timeout_secs=30, cancel_pending=True)
```

### Task Timeout
Automatically cancel tasks that run too long:
```python
@mp.parallel
def long_task():
    time.sleep(100)

# Automatically cancelled after 5 seconds
handle = long_task(timeout=5.0)
```

### Task Metadata
Track custom data for monitoring and debugging:
```python
handle = process_data(user_id, data)
handle.set_metadata('user_id', str(user_id))
handle.set_metadata('request_id', request_id)

# Later
metadata = handle.get_all_metadata()
logger.info(f"Processing user {metadata['user_id']}")
```

### Thread Pool Configuration
Configure the size and behavior of the internal thread pool:
```python
import makeParallel as mp

# Configure thread pool
mp.configure_thread_pool(num_threads=8)

# Check configuration
info = mp.get_thread_pool_info()
print(info)
```

### Priority Queues
Execute tasks based on priority levels:
```python
@mp.parallel_priority
def important_task(data):
    return process(data)

# Higher priority executes first
urgent = important_task(data1, priority=100)
normal = important_task(data2, priority=50)
low = important_task(data3, priority=10)
```

### Enhanced Task Cancellation
Better control over task cancellation:
```python
handle = long_task()

# Cancel with timeout
if handle.cancel_with_timeout(2.0):
    print("Cancelled successfully")

# Check cancellation status
if handle.is_cancelled():
    print("Task was cancelled")

# Monitor elapsed time
print(f"Elapsed: {handle.elapsed_time():.2f}s")

# Get task name
print(f"Running: {handle.get_name()}")
```

### Performance Profiling
Automatic performance tracking:
```python
@mp.profiled
def tracked_function(n):
    return expensive_computation(n)

# Run multiple times
for i in range(100):
    tracked_function(i)

# Get metrics
metrics = mp.get_metrics("tracked_function")
print(f"Average time: {metrics.average_execution_time_ms:.2f}ms")
print(f"Success rate: {metrics.completed_tasks / metrics.total_tasks * 100}%")

# Get all metrics
all_metrics = mp.get_all_metrics()
```

See [NEW_FEATURES.md](docs/NEW_FEATURES.md) for complete documentation.

## â­ Star History

If you find this useful, please â­ star the repo!

---

**Made with â¤ï¸ using Rust and Python**
